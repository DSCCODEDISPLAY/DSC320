{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "88aae6bb-311b-4f5e-897e-a369038ccb09",
      "cell_type": "code",
      "source": "#Step 1: Calculating Probabilities using Bayes’ Theorem\nP_A = 0.5\nP_B = 0.3\nP_C = 0.2\n\nP_def_given_A = 0.03\nP_def_given_B = 0.02\nP_def_given_C = 0.04\n\nP_def = (P_def_given_A * P_A) + (P_def_given_B * P_B) + (P_def_given_C * P_C)\nP_not_def = 1 - P_def\n\n#Step 1(a): If a randomly chosen graphics card is defective, what is the probability it was manufactured using Process A?\nP_A_given_def = (P_def_given_A * P_A) / P_def\nprint(f\"P(A | Defective): {P_A_given_def:.2%}\")\n\n#Step 1(b): If a randomly chosen graphics card is not defective, what is the probability it was manufactured using Process C?\nP_not_def_given_C = 0.96\nP_C_given_not_def = (P_not_def_given_C * P_C) / P_not_def\nprint(f\"P(C | Not Defective): {P_C_given_not_def:.2%}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "P(A | Defective): 51.72%\nP(C | Not Defective): 19.77%\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 1
    },
    {
      "id": "c22b5355-e52b-40e2-8703-a95b1d09c03d",
      "cell_type": "code",
      "source": "import numpy as np\n\n#Step 2: Entropy Function for a Probability Distribution\ndef calculate_entropy(probabilities):\n    probabilities = np.array(probabilities)\n    if not np.isclose(np.sum(probabilities), 1.0):\n        raise ValueError(\"The sum of all probabilities must equal 1.\")\n    if np.any(probabilities <= 0):\n        raise ValueError(\"All probabilities must be positive numbers greater than 0.\")\n    entropy = -np.sum(probabilities * np.log2(probabilities))\n    return entropy\n\n\n#Step 3: Calculating Entropies of Probability Distributions\ndef compare_entropies():\n    probabilities_X = [0.2, 0.2, 0.2, 0.2, 0.2]\n    probabilities_Y = [0.1, 0.4, 0.1, 0.3, 0.1]\n\n    entropy_X = calculate_entropy(probabilities_X)\n    entropy_Y = calculate_entropy(probabilities_Y)\n\n    print(\"Step 3(a):\")\n    print(f\"• Entropy of X: {entropy_X:.4f}\")\n    print(f\"• Entropy of Y: {entropy_Y:.4f}\")\n\n    if entropy_X > entropy_Y:\n        print(\"\\nStep 3(b):\")\n        print(\"Entropy of X is higher because its probabilities are uniformly distributed, indicating maximum uncertainty.\")\n    else:\n        print(\"\\nStep 3(b):\")\n        print(\"Entropy of Y is higher because its probabilities are unevenly distributed, reflecting less uncertainty.\")\n\ncompare_entropies()",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Step 3(a):\n• Entropy of X: 2.3219\n• Entropy of Y: 2.0464\n\nStep 3(b):\nEntropy of X is higher because its probabilities are uniformly distributed, indicating maximum uncertainty.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 2
    }
  ]
}